{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data.preprocessor2 as pre\n",
    "import src.data.datasets2 as data\n",
    "from src.model.network import create_model, quantile_loss, create_MQDCNN, MultiQuantileLoss\n",
    "from src.utils import compute_coverage_len, plot_quantiles, compute_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#in order not to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "removable_cols = [\"sm01\", \"sm05\", \"sm06\", \"sm10\", \"sm16\", \"sm18\", \"sm19\"]\n",
    "ignore_columns = [\"time\", \"os1\", \"os2\", \"os3\"]\n",
    "\n",
    "dataset = data.get_dataset(\"CMAPSS4\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "split_dataset = pre.split_dataset(dataset, calval_size=0.2, random_state=0)\n",
    "proc_dataset = pre.preprocess_split(split_dataset[\"train_split\"], split_dataset[\"scaler_factory\"], window_size=split_dataset[\"window_size\"], removable_cols=removable_cols, ignore_columns=ignore_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   9,  13,  23,  38,  45,  46,  56,  64,  65,  74,  75,  76,\n",
       "        77,  84,  90,  93,  97, 108, 110, 112, 117, 119, 126, 130, 131,\n",
       "       136, 146, 151, 157, 160, 162, 163, 169, 174, 177, 179, 190, 202,\n",
       "       203, 204, 208, 214, 219, 225, 227, 230, 237, 243, 249], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(proc_dataset[\"test\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   7,   8,  10,  11,  12,  14,  15,  16,\n",
       "        17,  18,  19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  30,\n",
       "        31,  32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  43,  44,\n",
       "        47,  48,  49,  50,  51,  52,  53,  54,  55,  57,  58,  59,  60,\n",
       "        61,  62,  63,  66,  67,  68,  69,  70,  71,  72,  73,  78,  79,\n",
       "        80,  81,  82,  83,  85,  86,  87,  88,  89,  91,  92,  94,  95,\n",
       "        96,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 111,\n",
       "       113, 114, 115, 116, 118, 120, 121, 122, 123, 124, 125, 127, 128,\n",
       "       129, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "       145, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 159, 161,\n",
       "       164, 165, 166, 167, 168, 170, 171, 172, 173, 175, 176, 178, 180,\n",
       "       181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211,\n",
       "       212, 213, 215, 216, 217, 218, 220, 221, 222, 223, 224, 226, 228,\n",
       "       229, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244,\n",
       "       245, 246, 247, 248], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(proc_dataset[\"train\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45175, 15, 14, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12588, 15, 14, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"test\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57763"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"X\"].shape[0] + proc_dataset[\"test\"][\"X\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 7581.2261 - root_mean_squared_error: 87.0702\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 6047.1953 - root_mean_squared_error: 77.7637\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 5015.0483 - root_mean_squared_error: 70.8170\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - 8s 94ms/step - loss: 4191.4326 - root_mean_squared_error: 64.7413\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 3539.0381 - root_mean_squared_error: 59.4898\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 3024.1428 - root_mean_squared_error: 54.9922\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 2609.4475 - root_mean_squared_error: 51.0828\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - 8s 93ms/step - loss: 2263.2009 - root_mean_squared_error: 47.5731\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - 8s 94ms/step - loss: 1976.7482 - root_mean_squared_error: 44.4606\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 1759.7627 - root_mean_squared_error: 41.9495\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 1602.8322 - root_mean_squared_error: 40.0354\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 1483.4943 - root_mean_squared_error: 38.5162\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - 8s 95ms/step - loss: 1392.9706 - root_mean_squared_error: 37.3225\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - 10s 117ms/step - loss: 1332.1656 - root_mean_squared_error: 36.4988\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - 8s 90ms/step - loss: 1294.5350 - root_mean_squared_error: 35.9796\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - 8s 90ms/step - loss: 1245.2618 - root_mean_squared_error: 35.2883\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 1215.6173 - root_mean_squared_error: 34.8657\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 1202.3605 - root_mean_squared_error: 34.6751\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 1182.8253 - root_mean_squared_error: 34.3922\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 1166.6652 - root_mean_squared_error: 34.1565\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 1150.6670 - root_mean_squared_error: 33.9215\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 1133.8582 - root_mean_squared_error: 33.6728\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 1110.2711 - root_mean_squared_error: 33.3207\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 1097.7740 - root_mean_squared_error: 33.1327\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - 8s 96ms/step - loss: 1068.0649 - root_mean_squared_error: 32.6813\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 1041.6708 - root_mean_squared_error: 32.2749\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 1012.0635 - root_mean_squared_error: 31.8129\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - 9s 97ms/step - loss: 984.7489 - root_mean_squared_error: 31.3807\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 948.5255 - root_mean_squared_error: 30.7981\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 889.2626 - root_mean_squared_error: 29.8205\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - 9s 98ms/step - loss: 830.4507 - root_mean_squared_error: 28.8175\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - 9s 101ms/step - loss: 766.1669 - root_mean_squared_error: 27.6797\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - 9s 96ms/step - loss: 702.5710 - root_mean_squared_error: 26.5061\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - 9s 96ms/step - loss: 649.7441 - root_mean_squared_error: 25.4901\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - 9s 100ms/step - loss: 603.2227 - root_mean_squared_error: 24.5606\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - 9s 106ms/step - loss: 566.1351 - root_mean_squared_error: 23.7936\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - 9s 99ms/step - loss: 544.2294 - root_mean_squared_error: 23.3287\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - 9s 103ms/step - loss: 516.2166 - root_mean_squared_error: 22.7204\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - 10s 114ms/step - loss: 511.5836 - root_mean_squared_error: 22.6182\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - 8s 92ms/step - loss: 482.5244 - root_mean_squared_error: 21.9664\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 478.8429 - root_mean_squared_error: 21.8825\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 465.5975 - root_mean_squared_error: 21.5777\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 467.3892 - root_mean_squared_error: 21.6192\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 441.7265 - root_mean_squared_error: 21.0173\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 443.1455 - root_mean_squared_error: 21.0510\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 431.6696 - root_mean_squared_error: 20.7767\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 422.8711 - root_mean_squared_error: 20.5638\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 428.2924 - root_mean_squared_error: 20.6952\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 420.0703 - root_mean_squared_error: 20.4956\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 408.1591 - root_mean_squared_error: 20.2029\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 412.9823 - root_mean_squared_error: 20.3220\n",
      "Epoch 52/100\n",
      "89/89 [==============================] - 8s 87ms/step - loss: 406.9873 - root_mean_squared_error: 20.1739\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 407.5323 - root_mean_squared_error: 20.1874\n",
      "Epoch 54/100\n",
      "89/89 [==============================] - 8s 91ms/step - loss: 393.8480 - root_mean_squared_error: 19.8456\n",
      "Epoch 55/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 394.6303 - root_mean_squared_error: 19.8653\n",
      "Epoch 56/100\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 394.3083 - root_mean_squared_error: 19.8572\n",
      "Epoch 57/100\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 390.2463 - root_mean_squared_error: 19.7547\n",
      "Epoch 58/100\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 390.1681 - root_mean_squared_error: 19.7527\n",
      "Epoch 59/100\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 394.0350 - root_mean_squared_error: 19.8503\n",
      "Epoch 60/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 382.3903 - root_mean_squared_error: 19.5548\n",
      "Epoch 61/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 376.5034 - root_mean_squared_error: 19.4037\n",
      "Epoch 62/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 381.1299 - root_mean_squared_error: 19.5225\n",
      "Epoch 63/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 375.7727 - root_mean_squared_error: 19.3849\n",
      "Epoch 64/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 375.5468 - root_mean_squared_error: 19.3790\n",
      "Epoch 65/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 378.7249 - root_mean_squared_error: 19.4609\n",
      "Epoch 66/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 366.6120 - root_mean_squared_error: 19.1471\n",
      "Epoch 67/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 377.5154 - root_mean_squared_error: 19.4298\n",
      "Epoch 68/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 366.6627 - root_mean_squared_error: 19.1484\n",
      "Epoch 69/100\n",
      "89/89 [==============================] - 7s 84ms/step - loss: 365.4085 - root_mean_squared_error: 19.1157\n",
      "Epoch 70/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 368.8358 - root_mean_squared_error: 19.2051\n",
      "Epoch 71/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 362.2871 - root_mean_squared_error: 19.0338\n",
      "Epoch 72/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 363.9293 - root_mean_squared_error: 19.0769\n",
      "Epoch 73/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 364.5426 - root_mean_squared_error: 19.0930\n",
      "Epoch 74/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 359.3794 - root_mean_squared_error: 18.9573\n",
      "Epoch 75/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 358.2849 - root_mean_squared_error: 18.9284\n",
      "Epoch 76/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 359.5078 - root_mean_squared_error: 18.9607\n",
      "Epoch 77/100\n",
      "89/89 [==============================] - 7s 82ms/step - loss: 357.3679 - root_mean_squared_error: 18.9042\n",
      "Epoch 78/100\n",
      "89/89 [==============================] - 7s 83ms/step - loss: 351.6882 - root_mean_squared_error: 18.7534\n",
      "Epoch 79/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 357.3261 - root_mean_squared_error: 18.9031\n",
      "Epoch 80/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 350.3045 - root_mean_squared_error: 18.7164\n",
      "Epoch 81/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 352.4362 - root_mean_squared_error: 18.7733\n",
      "Epoch 82/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 349.6665 - root_mean_squared_error: 18.6994\n",
      "Epoch 83/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 348.2107 - root_mean_squared_error: 18.6604\n",
      "Epoch 84/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 347.3348 - root_mean_squared_error: 18.6369\n",
      "Epoch 85/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 347.9754 - root_mean_squared_error: 18.6541\n",
      "Epoch 86/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 342.7393 - root_mean_squared_error: 18.5132\n",
      "Epoch 87/100\n",
      "89/89 [==============================] - 7s 80ms/step - loss: 346.3997 - root_mean_squared_error: 18.6118\n",
      "Epoch 88/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 345.0992 - root_mean_squared_error: 18.5768\n",
      "Epoch 89/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 338.1587 - root_mean_squared_error: 18.3891\n",
      "Epoch 90/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 338.5641 - root_mean_squared_error: 18.4001\n",
      "Epoch 91/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 341.9447 - root_mean_squared_error: 18.4917\n",
      "Epoch 92/100\n",
      "89/89 [==============================] - 7s 81ms/step - loss: 337.4761 - root_mean_squared_error: 18.3705\n",
      "Epoch 93/100\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 340.5388 - root_mean_squared_error: 18.4537\n",
      "Epoch 94/100\n",
      "89/89 [==============================] - 8s 86ms/step - loss: 338.6317 - root_mean_squared_error: 18.4019\n",
      "Epoch 95/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 336.8367 - root_mean_squared_error: 18.3531\n",
      "Epoch 96/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 334.3704 - root_mean_squared_error: 18.2858\n",
      "Epoch 97/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 334.8071 - root_mean_squared_error: 18.2977\n",
      "Epoch 98/100\n",
      "89/89 [==============================] - 8s 88ms/step - loss: 338.8766 - root_mean_squared_error: 18.4086\n",
      "Epoch 99/100\n",
      "89/89 [==============================] - 8s 85ms/step - loss: 335.3268 - root_mean_squared_error: 18.3119\n",
      "Epoch 100/100\n",
      "89/89 [==============================] - 8s 89ms/step - loss: 329.9717 - root_mean_squared_error: 18.1651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d200a5ed0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "epochs = 100 # training epochs\n",
    "optimizer = Adam(learning_rate=1e-3) # model optimizer\n",
    "\n",
    "X_train = proc_dataset[\"train\"][\"X\"]\n",
    "y_train = proc_dataset[\"train\"][\"y\"]\n",
    "DCNN = create_model(window_size=split_dataset[\"window_size\"], feature_dim=14, kernel_size=(10, 1), filter_num=10, dropout_rate=0.5)\n",
    "DCNN.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n",
    "DCNN.fit(x=X_train, y=y_train, batch_size = 512, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx, cal_idx = pre.split_by_group(X=proc_dataset[\"test\"][\"X\"], groups=proc_dataset[\"test\"][\"id\"], n_splits=1, test_size=0.5, random_state=5)\n",
    "X_val, X_cal = proc_dataset[\"test\"][\"X\"][val_idx], proc_dataset[\"test\"][\"X\"][cal_idx]\n",
    "y_val, y_cal = proc_dataset[\"test\"][\"y\"][val_idx], proc_dataset[\"test\"][\"y\"][cal_idx]\n",
    "idx_val, idx_cal = proc_dataset[\"test\"][\"index\"][val_idx], proc_dataset[\"test\"][\"index\"][cal_idx]\n",
    "id_val, id_cal = proc_dataset[\"test\"][\"id\"][val_idx], proc_dataset[\"test\"][\"id\"][cal_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,  13,  23,  38,  45,  46,  74,  97, 110, 117, 126, 130, 136,\n",
       "       151, 157, 162, 163, 169, 174, 190, 208, 214, 219, 225, 243],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(id_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,  56,  64,  65,  75,  76,  77,  84,  90,  93, 108, 112, 119,\n",
       "       131, 146, 160, 177, 179, 202, 203, 204, 227, 230, 237, 249],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_cal = DCNN.predict(x=X_cal, verbose=0)\n",
    "y_hat_val = DCNN.predict(x=X_val, verbose=0)\n",
    "\n",
    "scores = np.abs(y_cal - y_hat_cal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.15\n",
    "q = compute_quantile(scores, alpha)\n",
    "left_coverage, coverage, avg_length =  compute_coverage_len(y_val, y_hat_val-q, y_hat_val+q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.383856810628217"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight(rho, idx_val, idx_cal):\n",
    "    return rho**np.abs(idx_val - idx_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.9999\n",
    "\n",
    "sorted_scores_idx = scores.argsort(axis=0)\n",
    "sorted_scores = scores[sorted_scores_idx]\n",
    "\n",
    "q_list = []\n",
    "pos_list = []\n",
    "for i in idx_val: \n",
    "\n",
    "    weights = calculate_weight(rho, i, idx_cal[sorted_scores_idx]) \n",
    "    weights_normalized = weights/(weights.sum()+1)\n",
    "    # print(np.where(weights_normalized.cumsum() >= 1-alpha))\n",
    "    # weights_normalized = np.ones((1000))*1/1001\n",
    "    pos_i = np.where(weights_normalized.cumsum() >= 1-alpha)[0][0]\n",
    "    qi = sorted_scores[pos_i]\n",
    "    # print(np.where(weights_normalized.cumsum() >= 1-alpha)[0][0])\n",
    "    q_list.append(qi[0][0])\n",
    "    pos_list.append(pos_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_array = np.array(q_list).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 320.,  333.,  369., 1820.,  831.,  672.,  775.,  173.,  173.,\n",
       "         645.]),\n",
       " array([22.518265, 23.4295  , 24.340736, 25.251972, 26.163208, 27.074444,\n",
       "        27.98568 , 28.896915, 29.808151, 30.719387, 31.630623],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_coverage, coverage, avg_length =  compute_coverage_len(y_val, y_hat_val - q_array, y_hat_val + q_array)\n",
    "left_coverage_SCP, coverage_SCP, avg_length_SCP =  compute_coverage_len(y_val, y_hat_val - q, y_hat_val + q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SCP: Left coverage:0.9157257404680085, coverage:0.808214694812633, and the average length: 52.76771545410156\n",
      "For nex-SCP: Left coverage:0.9171984945180821, coverage:0.8093601701849125, and the average length: 53.82615280151367\n"
     ]
    }
   ],
   "source": [
    "print(f\"For SCP: Left coverage:{left_coverage_SCP}, coverage:{coverage_SCP}, and the average length: {avg_length_SCP}\")\n",
    "print(f\"For nex-SCP: Left coverage:{left_coverage}, coverage:{coverage}, and the average length: {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "SCP = [] #Split conformal prediction\n",
    "nex_SCP = [] #non-exchangeable Split conformal prediction\n",
    "alpha = 0.15\n",
    "\n",
    "lens = []\n",
    "qs = []\n",
    "q_lows = []\n",
    "q_highs = []\n",
    "\n",
    "\n",
    "for r in range(R):\n",
    "    val_idx, cal_idx = pre.split_by_group(X=proc_dataset[\"test\"][\"X\"], groups=proc_dataset[\"test\"][\"id\"], n_splits=1, test_size=0.5, random_state=r)\n",
    "    X_val, X_cal = proc_dataset[\"test\"][\"X\"][val_idx], proc_dataset[\"test\"][\"X\"][cal_idx]\n",
    "    y_val, y_cal = proc_dataset[\"test\"][\"y\"][val_idx], proc_dataset[\"test\"][\"y\"][cal_idx]\n",
    "    idx_val, idx_cal = proc_dataset[\"test\"][\"index\"][val_idx], proc_dataset[\"test\"][\"index\"][cal_idx]\n",
    "    id_val, id_cal = proc_dataset[\"test\"][\"id\"][val_idx], proc_dataset[\"test\"][\"id\"][cal_idx]\n",
    "\n",
    "    y_hat_cal = DCNN.predict(x=X_cal, verbose=0)\n",
    "    y_hat_val = DCNN.predict(x=X_val, verbose=0)\n",
    "\n",
    "    scores = np.abs(y_cal - y_hat_cal) \n",
    "\n",
    "\n",
    "    y_hat_calib = MQDCNN.predict(x=X_calib, verbose=0)\n",
    "    y_hat_val = MQDCNN.predict(x=X_val, verbose=0)\n",
    "\n",
    "    scores_low = y_hat_calib[0] - y_calib\n",
    "    scores_high = y_calib - y_hat_calib[1] \n",
    "    scores = np.maximum(scores_low, scores_high)\n",
    "    q = compute_quantile(scores, alpha)\n",
    "    q_low = compute_quantile(scores_low, alpha_low)\n",
    "    q_high = compute_quantile(scores_high, alpha_high)\n",
    "\n",
    "    lens.append(y_hat_val[1] - y_hat_val[0])\n",
    "    qs.append(q)\n",
    "    q_lows.append(q_low)\n",
    "    q_highs.append(q_high)\n",
    "    q = compute_quantile(scores, alpha)\n",
    "    left_coverage, coverage, avg_length =  \n",
    "\n",
    "    SCP.append(compute_coverage_len(y_val, y_hat_val-q, y_hat_val+q))\n",
    "    nex_SCP.append(compute_coverage_len(y_val, y_hat_val - q_array, y_hat_val + q_array))\n",
    "\n",
    "OQ_left_coverage, OQ_coverage, OQ_avg_length = zip(*OQ)\n",
    "CQ_left_coverage, CQ_coverage, CQ_avg_length = zip(*CQ)\n",
    "CQLR_left_coverage, CQLR_coverage, CQLR_avg_length = zip(*CQLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "epochs = 100 # training epochs\n",
    "optimizer = Adam(learning_rate=1e-3) # model optimizer\n",
    "calval_size = 2000 # 2000 of training data is used for cailbration and validation\n",
    "calib_size = 1000 # 1000 of cailbration+validation data is used for cailbration\n",
    "val_size = calval_size - calib_size # 1000 of cailbration+validation data is used for validation\n",
    "\n",
    "\n",
    "X_train, X_calval, y_train, y_calval, idx_train, idx_calval = \n",
    "\n",
    "X_train, X_calval, y_train, y_calval, idx_train, idx_calval = train_test_split(X, y, idx, test_size=calval_size, random_state=0)\n",
    "DCNN = create_model(window_size=30, feature_dim=14, kernel_size=(10, 1), filter_num=10, dropout_rate=0.5)\n",
    "DCNN.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n",
    "DCNN.fit(x=X_train, y=y_train, batch_size = 512, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2258, 30, 14, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(2258, 30, 14, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alireza\\Desktop\\projects\\Uncertainty quantification for RUL\\Uncertainty-quantification-for-remaining-useful-lifetime-estimation\\test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alireza/Desktop/projects/Uncertainty%20quantification%20for%20RUL/Uncertainty-quantification-for-remaining-useful-lifetime-estimation/test.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alireza/Desktop/projects/Uncertainty%20quantification%20for%20RUL/Uncertainty-quantification-for-remaining-useful-lifetime-estimation/test.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(proc_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\.conda\\envs\\rul_unc\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             index,\n\u001b[0;32m    697\u001b[0m             columns,\n\u001b[0;32m    698\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    699\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    700\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\.conda\\envs\\rul_unc\\lib\\site-packages\\pandas\\core\\internals\\construction.py:331\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    326\u001b[0m         values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarray(values, copy\u001b[39m=\u001b[39;49mcopy_on_sanitize)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(values\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[0;32m    334\u001b[0m     shape \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\.conda\\envs\\rul_unc\\lib\\site-packages\\pandas\\core\\internals\\construction.py:591\u001b[0m, in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    589\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m    590\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(2258, 30, 14, 1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(proc_dataset[\"test\"][\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>os1</th>\n",
       "      <th>os2</th>\n",
       "      <th>os3</th>\n",
       "      <th>sm01</th>\n",
       "      <th>sm02</th>\n",
       "      <th>sm03</th>\n",
       "      <th>sm04</th>\n",
       "      <th>sm05</th>\n",
       "      <th>...</th>\n",
       "      <th>sm13</th>\n",
       "      <th>sm14</th>\n",
       "      <th>sm15</th>\n",
       "      <th>sm16</th>\n",
       "      <th>sm17</th>\n",
       "      <th>sm18</th>\n",
       "      <th>sm19</th>\n",
       "      <th>sm20</th>\n",
       "      <th>sm21</th>\n",
       "      <th>rul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.26</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  time     os1     os2    os3    sm01    sm02     sm03     sm04  \\\n",
       "0        1     1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n",
       "1        1     2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n",
       "2        1     3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n",
       "3        1     4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n",
       "4        1     5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n",
       "...    ...   ...     ...     ...    ...     ...     ...      ...      ...   \n",
       "20626  100   196 -0.0004 -0.0003  100.0  518.67  643.49  1597.98  1428.63   \n",
       "20627  100   197 -0.0016 -0.0005  100.0  518.67  643.54  1604.50  1433.58   \n",
       "20628  100   198  0.0004  0.0000  100.0  518.67  643.42  1602.46  1428.18   \n",
       "20629  100   199 -0.0011  0.0003  100.0  518.67  643.23  1605.26  1426.53   \n",
       "20630  100   200 -0.0032 -0.0005  100.0  518.67  643.85  1600.38  1432.14   \n",
       "\n",
       "        sm05  ...     sm13     sm14    sm15  sm16  sm17  sm18   sm19   sm20  \\\n",
       "0      14.62  ...  2388.02  8138.62  8.4195  0.03   392  2388  100.0  39.06   \n",
       "1      14.62  ...  2388.07  8131.49  8.4318  0.03   392  2388  100.0  39.00   \n",
       "2      14.62  ...  2388.03  8133.23  8.4178  0.03   390  2388  100.0  38.95   \n",
       "3      14.62  ...  2388.08  8133.83  8.3682  0.03   392  2388  100.0  38.88   \n",
       "4      14.62  ...  2388.04  8133.80  8.4294  0.03   393  2388  100.0  38.90   \n",
       "...      ...  ...      ...      ...     ...   ...   ...   ...    ...    ...   \n",
       "20626  14.62  ...  2388.26  8137.60  8.4956  0.03   397  2388  100.0  38.49   \n",
       "20627  14.62  ...  2388.22  8136.50  8.5139  0.03   395  2388  100.0  38.30   \n",
       "20628  14.62  ...  2388.24  8141.05  8.5646  0.03   398  2388  100.0  38.44   \n",
       "20629  14.62  ...  2388.23  8139.29  8.5389  0.03   395  2388  100.0  38.29   \n",
       "20630  14.62  ...  2388.26  8137.33  8.5036  0.03   396  2388  100.0  38.37   \n",
       "\n",
       "          sm21  rul  \n",
       "0      23.4190  191  \n",
       "1      23.4236  190  \n",
       "2      23.3442  189  \n",
       "3      23.3739  188  \n",
       "4      23.4044  187  \n",
       "...        ...  ...  \n",
       "20626  22.9735    4  \n",
       "20627  23.1594    3  \n",
       "20628  22.9333    2  \n",
       "20629  23.0640    1  \n",
       "20630  23.0522    0  \n",
       "\n",
       "[20631 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12573, 30, 14, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12573, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 95, 95, ..., 39, 39, 39], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11255, 11271, 11331, ..., 19961, 19974, 19985], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"train\"][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3290, 30, 14, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_dataset[\"test\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset[\"train\"].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7, 7, 7, 7]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33727,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.hstack([a,b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.preprocess_split(dataset, **dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=lambda d: d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'CMAPSS1',\n",
       " 'train':         id  time     os1     os2    os3    sm01    sm02     sm03     sm04  \\\n",
       " 0        1     1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n",
       " 1        1     2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n",
       " 2        1     3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n",
       " 3        1     4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n",
       " 4        1     5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n",
       " ...    ...   ...     ...     ...    ...     ...     ...      ...      ...   \n",
       " 20626  100   196 -0.0004 -0.0003  100.0  518.67  643.49  1597.98  1428.63   \n",
       " 20627  100   197 -0.0016 -0.0005  100.0  518.67  643.54  1604.50  1433.58   \n",
       " 20628  100   198  0.0004  0.0000  100.0  518.67  643.42  1602.46  1428.18   \n",
       " 20629  100   199 -0.0011  0.0003  100.0  518.67  643.23  1605.26  1426.53   \n",
       " 20630  100   200 -0.0032 -0.0005  100.0  518.67  643.85  1600.38  1432.14   \n",
       " \n",
       "         sm05  ...     sm13     sm14    sm15  sm16  sm17  sm18   sm19   sm20  \\\n",
       " 0      14.62  ...  2388.02  8138.62  8.4195  0.03   392  2388  100.0  39.06   \n",
       " 1      14.62  ...  2388.07  8131.49  8.4318  0.03   392  2388  100.0  39.00   \n",
       " 2      14.62  ...  2388.03  8133.23  8.4178  0.03   390  2388  100.0  38.95   \n",
       " 3      14.62  ...  2388.08  8133.83  8.3682  0.03   392  2388  100.0  38.88   \n",
       " 4      14.62  ...  2388.04  8133.80  8.4294  0.03   393  2388  100.0  38.90   \n",
       " ...      ...  ...      ...      ...     ...   ...   ...   ...    ...    ...   \n",
       " 20626  14.62  ...  2388.26  8137.60  8.4956  0.03   397  2388  100.0  38.49   \n",
       " 20627  14.62  ...  2388.22  8136.50  8.5139  0.03   395  2388  100.0  38.30   \n",
       " 20628  14.62  ...  2388.24  8141.05  8.5646  0.03   398  2388  100.0  38.44   \n",
       " 20629  14.62  ...  2388.23  8139.29  8.5389  0.03   395  2388  100.0  38.29   \n",
       " 20630  14.62  ...  2388.26  8137.33  8.5036  0.03   396  2388  100.0  38.37   \n",
       " \n",
       "           sm21  rul  \n",
       " 0      23.4190  191  \n",
       " 1      23.4236  190  \n",
       " 2      23.3442  189  \n",
       " 3      23.3739  188  \n",
       " 4      23.4044  187  \n",
       " ...        ...  ...  \n",
       " 20626  22.9735    4  \n",
       " 20627  23.1594    3  \n",
       " 20628  22.9333    2  \n",
       " 20629  23.0640    1  \n",
       " 20630  23.0522    0  \n",
       " \n",
       " [20631 rows x 27 columns],\n",
       " 'test':         id  time     os1     os2    os3    sm01    sm02     sm03     sm04  \\\n",
       " 0        1     1  0.0023  0.0003  100.0  518.67  643.02  1585.29  1398.21   \n",
       " 1        1     2 -0.0027 -0.0003  100.0  518.67  641.71  1588.45  1395.42   \n",
       " 2        1     3  0.0003  0.0001  100.0  518.67  642.46  1586.94  1401.34   \n",
       " 3        1     4  0.0042  0.0000  100.0  518.67  642.44  1584.12  1406.42   \n",
       " 4        1     5  0.0014  0.0000  100.0  518.67  642.51  1587.19  1401.92   \n",
       " ...    ...   ...     ...     ...    ...     ...     ...      ...      ...   \n",
       " 13091  100   194  0.0049  0.0000  100.0  518.67  643.24  1599.45  1415.79   \n",
       " 13092  100   195 -0.0011 -0.0001  100.0  518.67  643.22  1595.69  1422.05   \n",
       " 13093  100   196 -0.0006 -0.0003  100.0  518.67  643.44  1593.15  1406.82   \n",
       " 13094  100   197 -0.0038  0.0001  100.0  518.67  643.26  1594.99  1419.36   \n",
       " 13095  100   198  0.0013  0.0003  100.0  518.67  642.95  1601.62  1424.99   \n",
       " \n",
       "         sm05  ...     sm13     sm14    sm15  sm16  sm17  sm18   sm19   sm20  \\\n",
       " 0      14.62  ...  2388.03  8125.55  8.4052  0.03   392  2388  100.0  38.86   \n",
       " 1      14.62  ...  2388.06  8139.62  8.3803  0.03   393  2388  100.0  39.02   \n",
       " 2      14.62  ...  2388.03  8130.10  8.4441  0.03   393  2388  100.0  39.08   \n",
       " 3      14.62  ...  2388.05  8132.90  8.3917  0.03   391  2388  100.0  39.00   \n",
       " 4      14.62  ...  2388.03  8129.54  8.4031  0.03   390  2388  100.0  38.99   \n",
       " ...      ...  ...      ...      ...     ...   ...   ...   ...    ...    ...   \n",
       " 13091  14.62  ...  2388.00  8213.28  8.4715  0.03   394  2388  100.0  38.65   \n",
       " 13092  14.62  ...  2388.09  8210.85  8.4512  0.03   395  2388  100.0  38.57   \n",
       " 13093  14.62  ...  2388.04  8217.24  8.4569  0.03   395  2388  100.0  38.62   \n",
       " 13094  14.62  ...  2388.08  8220.48  8.4711  0.03   395  2388  100.0  38.66   \n",
       " 13095  14.62  ...  2388.05  8214.64  8.4903  0.03   396  2388  100.0  38.70   \n",
       " \n",
       "           sm21  rul  \n",
       " 0      23.3735  142  \n",
       " 1      23.3916  141  \n",
       " 2      23.4166  140  \n",
       " 3      23.3737  139  \n",
       " 4      23.4130  138  \n",
       " ...        ...  ...  \n",
       " 13091  23.1974   24  \n",
       " 13092  23.2771   23  \n",
       " 13093  23.2051   22  \n",
       " 13094  23.2699   21  \n",
       " 13095  23.1855   20  \n",
       " \n",
       " [13096 rows x 27 columns],\n",
       " 'scaler_factory': sklearn.preprocessing._data.StandardScaler,\n",
       " 'ignore_columns': ['os1', 'os2', 'os3']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(dataset, **g(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, calval_size=0.2):\n",
    "    \"\"\"split data into train and calibration+validation\n",
    "\n",
    "    Args:\n",
    "        dataset: a dictionary containing train and test dataframes and their scaler. \n",
    "        calval_size (float, optional):  the proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        a dictionary containing train-calval split, train, and test dataframes and their scaler. \n",
    "    \"\"\"\n",
    "    train_df = dataset[\"train\"]\n",
    "    test_df = dataset[\"test\"]\n",
    "\n",
    "    train, calval = train_test_split(train_df, test_size=calval_size, random_state=0, stratify=train_df[\"id\"])\n",
    "    train_split = dict(train=train, test=calval)\n",
    "\n",
    "    return {\n",
    "        **dataset,\n",
    "        \"train_split\": train_split,\n",
    "        \"train\": train_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "\n",
    "def map_split_dataset(\n",
    "    f: typing.Callable, dataset: SplitDataset,\n",
    "    g: typing.Callable = lambda _: dict()) -> SplitDataset:\n",
    "    return {\n",
    "        **dataset,\n",
    "        **f(dataset, **g(dataset)),\n",
    "        \"train_splits\": fy.lmap(\n",
    "            lambda s: f(s, **g(dataset)),\n",
    "            dataset[\"train_splits\"])\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('rul_unc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ed615bccc073c0a0281f121b591de3b0f9d3533c6b13e0905cf5e0bb75735d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
